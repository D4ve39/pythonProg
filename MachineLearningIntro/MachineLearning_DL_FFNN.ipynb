{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearning_DL-FFNN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D4ve39/pythonProg/blob/master/MachineLearning_DL_FFNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlIuN_8fNunb"
      },
      "source": [
        "## Machine Learning Workshop 2021 - Deep Learning\n",
        "# A gentle introduction to Neural Networks\n",
        "\n",
        "Today we will attempt to solve a machine learning problem by building and training a neural network. In particular, we will implement a very general architecture: a Multi Layer Perceptron (MLP). We will see how well it does on the task of handwritten digit recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhXGnwG_Nu3n"
      },
      "source": [
        "*Keras* will be our library of choice. It relies on a more powerful library (*tensorflow*) to handle the training process of neural networks, but it also exposes a very intuitive interface that will remind you of *sklearn*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFVyLQ45NzXd"
      },
      "source": [
        "# first of all, we import the libraries we will use\n",
        "\n",
        "import numpy as np  # to deal with matrix and numerical computation\n",
        "import matplotlib.pyplot as plt  # our loyal plotting library\n",
        "import pandas as pd  # great tool for managing datasets\n",
        "from sklearn.model_selection import train_test_split  # good old splitting function\n",
        "from sklearn import datasets\n",
        "\n",
        "# finally, our newcomers! tensorflow and keras\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFW_CZtJNzzA"
      },
      "source": [
        "## Data preparation\n",
        "As usual, we will load the data, split it and visualize it. Since neural networks are notoriously expensive to train, we will unfortunately not be able to rely on cross-validation. Instead, we will use an arguably suboptimal validation strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z7GB8W2N8rY"
      },
      "source": [
        "# let us fetch this datasets once again\n",
        "\n",
        "digits = datasets.fetch_openml('mnist_784', version=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpgB7q_HOBAJ"
      },
      "source": [
        "# preprocessing!\n",
        "\n",
        "data = digits.data  # get features\n",
        "data = data.astype(np.float32) / 255  # we normalize the data to avoid extreme values\n",
        "labels = digits.target  # get labels\n",
        "labels = keras.utils.to_categorical(labels)  # we are using one-hot labels to handle vectorized outputs\n",
        "\n",
        "# split training and test data\n",
        "\n",
        "training_data, test_data, training_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ayq9GAFOB5-"
      },
      "source": [
        "# Let us check some simple statistics\n",
        "print('data shape: {}'.format(training_data.shape[1:]))\n",
        "print('# training samples: {}'.format(training_data.shape[0]))\n",
        "print('# test samples: {}'.format(test_data.shape[0]))\n",
        "print('# features: {}'.format(training_data.shape[1]))\n",
        "print('# classes: {}'.format(training_labels.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmlxFTtvNz74"
      },
      "source": [
        "We can (and should) have a look at the training data! On the other hand, we should forget about the test dataset for now, in order to avoid information leakage.\n",
        "\n",
        "All data samples can be presented as an image (2D array of values) or flattened as a 1D vector. The first option is great for visualization, while the second is needed later as our MLP will only take 1D vectors as input.\n",
        "\n",
        "Similarly, the labels can be represented as an integer (from 0 to 9) or as a one-hot vector of size 10, which will match the output size of the MLP. You can see some examples below.\n",
        "\n",
        "Note: do not pay too much attention to the code for visualization, it is not too important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck_KWUbpOGmq"
      },
      "source": [
        "# visualize sample images from the training set\n",
        "images_to_plot = 4  # number of samples to show\n",
        "fig, ax = plt.subplots(images_to_plot, figsize=(5,15))\n",
        "rnd_id = np.random.randint(0,len(training_data),images_to_plot)  # pick a random index in the training set\n",
        "for i,id in enumerate(rnd_id):\n",
        "    rnd_img = training_data[id].reshape((28, 28))  # reshape from vector to matrix\n",
        "    rnd_lbl = training_labels[id]\n",
        "    ax[i].imshow(rnd_img)\n",
        "    ax[i].set_title(f'True label: {rnd_lbl}   ({rnd_lbl@np.arange(10)})')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdeJewg0OMe7"
      },
      "source": [
        "## Creating a model\n",
        "Now, let us turn to designing our neural network. For simplicity, we will start with the bare minimum: since the output will be a 10 dimensional vector (one element for each possible label), we will construct a network with only 10 neurons. This means that each output is then computed as a linear combination of input features. We really could not choose a simpler architecture.\n",
        "\n",
        "Note: the *softmax* activation will transform the outputs into a probability distribution over different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWiZlmgpOIin"
      },
      "source": [
        "n_features = training_data.shape[1]\n",
        "n_classes = 10\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(n_features,)),\n",
        "        layers.Dense(n_classes, activation='softmax')\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU5hC7kBOVAy"
      },
      "source": [
        "The model is thus created. Let us have a look at how many parameters/weights it will need to learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czia4zmkOQl_"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHtyHqXInqb_"
      },
      "source": [
        "We can even ask *keras* to plot our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moYGs04PnrF2"
      },
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imnKO2uOObMA"
      },
      "source": [
        "We have finally defined our neural network, which represents a model class. The last obstacle we need to overcome is training the network or, in other words, learning good parameters. This will not be too different from previous methods: we will define a goodness of fit criterion and try to optimize it. While this will be handled directly by *keras*, we will now take some time to introduce the main idea:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgNz0uRJOcDd"
      },
      "source": [
        "## Gradient Descent\n",
        "We start by defining a loss function. A loss function quantifies the inaccuracies of our model on the training data as a function of its parameters. Examples of loss functions are the Mean Square Error for regression, or Cross Entropy for classification, which is a generalization of the Binary Cross Entropy we have seen with logistic regression.\n",
        "\n",
        "Gradient Descent is an optimizer algorithm: it iteratively updates the parameters of the neural network to minimize the loss function. We will now see how gradient descent operates on a much simpler function, but the procedure for complex loss functions is not too dissimilar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIE0YCvMOW9-"
      },
      "source": [
        "# first of all, we introduce a simple quadratic function\n",
        "\n",
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "# x is the (only) parameter of this function\n",
        "# our goal is that of finding a value of x that minimizes f(x) (a minimum of f)\n",
        "# this corresponds to finding a set of parameters that minimizes the loss funciton in a neural network\n",
        "\n",
        "# gradient descent relies on one hyperparameter that controls the magnitude of parameter updates\n",
        "learning_rate = 0.2\n",
        "\n",
        "# first we initialize our parameter (usually would be done randomly)\n",
        "x = -1.9\n",
        "print('Initialization: {}'.format(x))\n",
        "\n",
        "# now we iterate n times\n",
        "n = 5\n",
        "\n",
        "history = [x]\n",
        "for i in range(n): \n",
        "    # we evaluate the function (forward pass)\n",
        "    y = f(x)\n",
        "    print(f'Step {i} - value: {y}')\n",
        "    \n",
        "    # we compute the derivative of f wrt our parameter x (gradient)\n",
        "    # we will do this by hand, but keras can automatically compute very complex gradients\n",
        "    grad = 2 * x\n",
        "    print(f'Step {i} - gradient: {grad}')\n",
        "    \n",
        "    # we now take a single optimization step\n",
        "    # the gradient points our the direction in which the function grows\n",
        "    # in our case, if the gradient is positive, it means that the function locally increases as x increases\n",
        "    # if the gradient is negative, the function decreases as x increases\n",
        "    # since we want to minimize f, we have to take a step in the opposite direction wrt to the gradient\n",
        "    \n",
        "    step = -grad * learning_rate\n",
        "    x = x + step\n",
        "    history.append(x)\n",
        "    print(f'Step {i} - parameter: {x}')\n",
        "    # ... and we repeat!\n",
        "\n",
        "\n",
        "# let us have a look at what happened\n",
        "\n",
        "z = np.linspace(-2, 2, 100)\n",
        "plt.plot(z, f(z))\n",
        "plt.plot(history, [f(x) for x in history], '-o')\n",
        "for i, x in enumerate(history):\n",
        "    plt.annotate(str(i), (x, f(x)), fontsize='large')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV9rXBVcOkwB"
      },
      "source": [
        "## Training\n",
        "\n",
        "Let us now let *keras* handle all of this. All we need to do is call the right methods.\n",
        "\n",
        "Note: at each gradient descent step the loss function is recomputed on part of the training set, called a batch. Once each batch that the training set was divided into has been used, we start over. Each complete pass over the dataset is called an epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_0iTi7NOiXQ"
      },
      "source": [
        "batch_size = 128  # how much data to feed in at once\n",
        "epochs = 20  # how many time to go over the dataset\n",
        "\n",
        "# define loss, optimizer and metrics\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.SGD(1e-2), metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(training_data, training_labels, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz7Nm_gVOnws"
      },
      "source": [
        "def show_history(h, show_plots=False):\n",
        "    if show_plots:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "        ax[0].plot(history.history['accuracy'], label='Training accuracy')\n",
        "        ax[0].plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "        ax[0].set_title('Model accuracy')\n",
        "        ax[0].set_ylabel('Accuracy')\n",
        "        ax[0].set_xlabel('Epoch')\n",
        "        ax[0].legend(loc='lower right')\n",
        "        ax[1].plot(history.history['loss'], label='Training loss')\n",
        "        ax[1].plot(history.history['val_loss'], label='Validation loss')\n",
        "        ax[1].set_title('Model loss')\n",
        "        ax[1].set_ylabel('Loss')\n",
        "        ax[1].set_xlabel('Epoch')\n",
        "        ax[1].legend(loc='upper right')\n",
        "        plt.show()\n",
        "    print('Final training accuracy: {}'.format(history.history['accuracy'][-1]))\n",
        "    print('Final validation accuracy: {}'.format(history.history['val_accuracy'][-1]))\n",
        "    print('Final training loss: {}'.format(history.history['loss'][-1]))\n",
        "    print('Final validation loss: {}'.format(history.history['val_loss'][-1]))\n",
        "\n",
        "show_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vNgZ5fZOsof"
      },
      "source": [
        "Results are not great...as a matter fact they are comparable with logistic regression! This is because such a simple model has basically the same expressivity as simpler methods method. We have to go deeper! Let us add more layer and (hopefully) increase the expressiveness of the network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03AZhwx4OtCq"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(n_features,)),\n",
        "        layers.Dense(256),  # additional layer!\n",
        "        layers.Dense(n_classes, activation='softmax'),\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhBHHdd0OuiV"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",  optimizer=keras.optimizers.SGD(1e-2), metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(training_data, training_labels, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVeZYhX9OwFC",
        "outputId": "c35f5b16-a621-481d-e992-db5925673b4d"
      },
      "source": [
        "show_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final training accuracy: 0.9158532023429871\n",
            "Final validation accuracy: 0.9200000166893005\n",
            "Final training loss: 0.29516375064849854\n",
            "Final validation loss: 0.28193774819374084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFLFiC_cOz6O"
      },
      "source": [
        "It is not improving much...why?\n",
        "Because all operations are completely linear. We need to introduce non-linearities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo4Se3gOO0st"
      },
      "source": [
        "## Finally, an actual MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dREhogzOx1y"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(n_features,)),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(n_classes, activation='softmax'),\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42-BLBbqO4rP"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",  optimizer=keras.optimizers.Adam(1e-2), metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(training_data, training_labels, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb6t_FQ0O7U8"
      },
      "source": [
        "show_history(history, show_plots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNH6WsJ6O_Iv"
      },
      "source": [
        "Now we are talking! Two ingredients are fundamental: multiple (or very wide) layers and non linearities.\n",
        "What we have achieved is called a MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCC8U2RTPBzY"
      },
      "source": [
        "## Your turn: Playtime!\n",
        "\n",
        "Now you can play around! In general, a model that is too complex will be more powerful, but harder to train and prone to overfitting. Can you strike the right balance? Try to customize the architecture and improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u15-zsZwPGRb"
      },
      "source": [
        "### a) Architecture\n",
        "\n",
        "Our MLP only has two layers. Can you add new ones? Can you increase the number of units per layer? Does that help?\n",
        "\n",
        "\n",
        "### b) Optimizer\n",
        "\n",
        "By default we are using a cousin of vanilla gradient descent called SGD. However, other fancier optimizers exist. What happens if you swap out 'sgd' for 'adam' or 'rmsprop' when compiling the model?\n",
        "\n",
        "\n",
        "### c) Batch size and epochs\n",
        "Try changing the default value of batch size and see how that affects training. You can do so by passing the right arguments to model.fit\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p5Xx0m8O9JR"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        # ...\n",
        "    ]\n",
        ")\n",
        "model.summary()\n",
        "model.compile(...)\n",
        "\n",
        "history = model.fit(training_data, training_labels, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "show_history(history, show_plots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doYyKtefoAmn"
      },
      "source": [
        "If you want, you can finally evaluate your model on test data to get a proper estimate of its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR34b-t9PIl5"
      },
      "source": [
        "score = model.evaluate(test_data, test_labels, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}